import serpapi
import os
import pandas as pd
import requests 
import json
from bs4 import BeautifulSoup 
from scrapy.crawler import CrawlerProcess
from web_scrapy.web_scrapy.spiders.spider1 import Spider1  # Replace with the actual import path to your spider

from dotenv import load_dotenv
import sqlite3

#for summarizing model
import torch
from transformers import pipeline 

from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline



params = {
    'api_key':'C7568B51036E41BCB8D7C4FCBD3D27F7',
    'q':'American Revolution',
    'engine':'google',
    'hl':'en',
}

url_list = []
url_list.append("https://en.wikipedia.org/wiki/American_Revolution")
def run_spider():
    # Create a CrawlerProcess instance
    process = CrawlerProcess()

    # Add the Spider1 spider to the process
    process.crawl(Spider1, start_urls=url_list)

    # Start the crawling process
    process.start()

if __name__ == "__main__":
    run_spider()

def summarize_source():
    conn = sqlite3.connect('example.db')  # Connect to the same database
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM websites')
    rows = cursor.fetchall()

    summarizer = pipeline(
        "summarization",
        "pszemraj/long-t5-tglobal-base-16384-book-summary",
        device=0 if torch.cuda.is_available() else -1,
    )
    result = summarizer(rows[0][1])
    print(result[0]["summary_text"])

def run_model_on_data():
    conn = sqlite3.connect('example.db')  # Connect to the same database
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM websites')
    rows = cursor.fetchall()
    

    model_name = "deepset/tinyroberta-squad2"

    # a) Get predictions
    nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
    QA_input = {
        'question': 'Who was Patrick Henry?',
        'context': rows[0][1]
    }
    res = nlp(QA_input)
    
    conn.close()
    
