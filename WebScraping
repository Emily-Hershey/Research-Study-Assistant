import serpapi
import os
import pandas as pd
import requests 
import json
from bs4 import BeautifulSoup 
from scrapy.crawler import CrawlerProcess
from web_scrapy.web_scrapy.spiders.spider1 import Spider1  # Replace with the actual import path to your spider

from dotenv import load_dotenv

params = {
    'api_key':'C7568B51036E41BCB8D7C4FCBD3D27F7',
    'q':'American Revolution',
    'engine':'google',
    'hl':'en',
}

api_result = requests.get('https://api.scaleserp.com/search', params)
response_data = api_result.json()
url_list = []
for item in response_data["organic_results"]:
    URL = item['link'] 
    url_list.append(URL)
def run_spider():
    # Create a CrawlerProcess instance
    process = CrawlerProcess()

    # Add the Spider1 spider to the process
    process.crawl(Spider1, start_urls=url_list)

    # Start the crawling process
    process.start()

if __name__ == "__main__":
    run_spider()
